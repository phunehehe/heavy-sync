#!/usr/bin/env python

from boto.exception import GSResponseError, S3ResponseError
from datetime import datetime, timedelta
from multiprocessing.pool import ThreadPool
from os import path, rename
from time import time

import argparse
import boto
import socket
import sqlite3
import sys
import tempfile

# Seemingly unused but needed for GCS authentication
import gcs_oauth2_boto_plugin


def get_bucket(scheme, bucket_name):
    connection = {
        'gs': boto.connect_gs,
        's3': boto.connect_s3,
    }[scheme]()
    return connection.get_bucket(bucket_name)


def delete_old_versions(bucket, folder, old_threshold):

    if old_threshold == 0:
        return
    assert old_threshold > 0

    print 'Purging old deleted objects...'

    limit = datetime.now() - timedelta(days=old_threshold)

    for k in bucket.list_versions(prefix=folder):
        if not k.DeletedTime:
            continue
        # This ignores timezones, but that doesn't matter now
        deleted_time = datetime.strptime(k.DeletedTime[0:-5],
                                         '%Y-%m-%dT%H:%M:%S')
        if deleted_time < limit:
            print 'Purging %s, deleted at %s' % (k.name, deleted_time)
            k.delete()


# Download an object from source bucket, then upload it to destination bucket
def transfer(source_bucket, destination_bucket, path):
    while True:
        try:
            # Roll over when hitting 10 MB
            f = tempfile.SpooledTemporaryFile(max_size=10*2**20)
            source_bucket.get_key(path).get_contents_to_file(f)
            destination_bucket.new_key(path).set_contents_from_file(f, rewind=True)
            return path
        except socket.error as e:
            sys.stderr.write(str(e))
            sys.stderr.write('Retrying path: %s%s' %
                             (source, path))
        except (GSResponseError, S3ResponseError) as e:
            if e.status == 404:
                sys.stderr.write(str(e))
                sys.stderr.write('Ignoring path: %s%s' %
                                 (source, path))
                return path
            else:
                raise


# Remove an object from a bucket, ignoring "not found" errors
def remove(bucket, path):
    try:
        bucket.delete_key(path)
    except (GSResponseError, S3ResponseError) as e:
        if e.status != 404:
            raise
        sys.stderr.write(str(e))
        sys.stderr.write('Ignoring path: %s%s' %
                         (destination, path))


def finished(connection):
    cursor = connection.cursor()
    cursor.execute('''SELECT 1 FROM source WHERE NOT processed LIMIT 1''')
    return cursor.fetchone() is None


def process(source_bucket, destination_bucket, connection):

    print 'Skipping over up-to-date objects...'
    connection.execute('''
        UPDATE source SET processed = 1
        WHERE rowid IN (
            SELECT s.rowid FROM source s JOIN destination d
            ON s.path = d.path AND s.hash = d.hash
        )
    ''')

    def transfer_row(row):
        return transfer(source_bucket, destination_bucket, row[0])

    print 'Uploading new/updated objects from source to destination...'
    while not finished(connection):
        sql_it = connection.execute('''SELECT path FROM source WHERE NOT processed LIMIT 1000''')
        pool = ThreadPool()
        pool_it = pool.imap_unordered(transfer_row, list(sql_it))
        for path in pool_it:
            connection.execute('''UPDATE source SET processed = 1 WHERE path = ?''', (path,))
            print 'Finished: %s/%s -> %s/%s' % (source_bucket, path, destination_bucket, path)

    print 'Deleting objects in destination that have been deleted in source...'
    for row in connection.execute('''
        SELECT d.rowid, d.path
        FROM destination d LEFT JOIN source s
        ON d.path = s.path WHERE s.path IS NULL
    '''):
        remove(destination_bucket, row[1])
        connection.execute('''DELETE FROM destination WHERE rowid = ?''', (row[0],))


# Populate the table with the contents of the bucket
def get_contents(bucket, folder, connection, table):
    for key in bucket.list(prefix=folder):
        connection.execute('INSERT INTO %s (bucket, path, hash) VALUES (?, ?, ?)' % table,
                           (bucket.name, key.name, key.etag))


def initialize_db(connection):
    connection.executescript('''
        CREATE TABLE source (bucket VARCHAR,
                             path VARCHAR,
                             hash VARCHAR,
                             processed BOOLEAN DEFAULT 0);
        CREATE TABLE destination (bucket VARCHAR, path VARCHAR, hash VARCHAR);
        CREATE INDEX IF NOT EXISTS source_path_index ON source (path);
        CREATE INDEX IF NOT EXISTS source_hash_index ON source (hash);
        CREATE INDEX IF NOT EXISTS destination_path_index ON destination (path);
        CREATE INDEX IF NOT EXISTS destination_hash_index ON destination (hash);
    ''')


def new_run(source_bucket, destination_bucket, folder, db):
    print 'Starting a new run...'

    connection = sqlite3.connect(db, isolation_level=None)
    initialize_db(connection)

    get_contents(destination_bucket, folder, connection, 'destination')
    get_contents(source_bucket, folder, connection, 'source')

    process(source_bucket, destination_bucket, connection)


def break_uri(uri):
    # scheme://bucket-name/sub/folder -> (scheme, bucket-name, /sub/folder)
    parts = uri.split('/')
    scheme = parts[0].split(':')[0]
    bucket_name = parts[2]
    folder = '/'.join(parts[3:])
    return (scheme, bucket_name, folder)


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('source')
    parser.add_argument('destination')
    parser.add_argument('--db', default='state.db')
    parser.add_argument('--old-threshold', default=0)
    args = parser.parse_args()

    s_scheme, s_bucket_name, folder = break_uri(args.source)
    d_scheme, d_bucket_name, _      = break_uri(args.destination)

    s_bucket = get_bucket(s_scheme, s_bucket_name)
    d_bucket = get_bucket(d_scheme, d_bucket_name)

    db = args.db

    if path.exists(db):

        try:
            connection = sqlite3.connect(db, isolation_level=None)
            if finished(connection):
                print 'Backing up previous completed run...'
                connection.close()
                rename(db, '%s-%d' % (db, int(time())))
                new_run(s_bucket, d_bucket, folder, db)
            else:
                print 'Resuming a previous run...'
                process(s_bucket, d_bucket, connection)

        except sqlite3.OperationalError as e:
            sys.stderr.write(str(e))
            sys.stderr.write('Error encountered, please clean up %s manually.'
                             % db)
            raise

    else:
        new_run(s_bucket, d_bucket, folder, db)

    delete_old_versions(d_bucket, folder, args.old_threshold)


main()
