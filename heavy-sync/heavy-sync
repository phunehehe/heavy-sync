#!/usr/bin/env python

from boto.exception import GSResponseError, S3ResponseError
from datetime import datetime, timedelta
from functools import partial
#from multiprocessing import Pool
from multiprocessing.pool import ThreadPool as Pool
from os import path, rename
from time import time

import argparse
import boto
import socket
import sqlite3
import sys
import tempfile

# Seemingly unused but needed for GCS authentication
import gcs_oauth2_boto_plugin


# Work around https://github.com/boto/boto/issues/2836
import ssl

_old_match_hostname = ssl.match_hostname

def _new_match_hostname(cert, hostname):
   if hostname.endswith('.s3.amazonaws.com'):
      pos = hostname.find('.s3.amazonaws.com')
      hostname = hostname[:pos].replace('.', '') + hostname[pos:]
   return _old_match_hostname(cert, hostname)

ssl.match_hostname = _new_match_hostname


def to_stderr(message):
    sys.stderr.write('%s\n' % message)


def get_connection(scheme):
    return {
        'gs': boto.connect_gs,
        's3': boto.connect_s3,
    }[scheme]()


def lookup(scheme, bucket_name):
    # Can't use `lookup` due to https://github.com/boto/boto/issues/2262 and
    # https://github.com/boto/boto/issues/2836
    try:
        return get_connection(scheme).get_bucket(bucket_name)
    except (GSResponseError, S3ResponseError) as e:
        if e.status == 404:
            return None
        else:
            raise


# Download an object from source bucket, then upload it to destination bucket
def transfer(source_bucket, destination_bucket, path):
    while True:
        try:
            # Roll over when hitting 10 MB
            f = tempfile.SpooledTemporaryFile(max_size=10*2**20)
            source_key = source_bucket.get_key(path)
            if source_key is None:
                to_stderr('Ignoring nonexistent path: %s/%s' % (
                    source_bucket, path))
                return path
            source_key.get_contents_to_file(f)
            destination_bucket.new_key(path).set_contents_from_file(f, rewind=True)
            return path
        except socket.error as e:
            to_stderr(str(e))
            to_stderr('Retrying path: %s/%s' % (
                source_bucket, path))
        except:
            to_stderr('Exception caught during transfer: %s/%s -> %s/%s' % (
                source_bucket, path, destination_bucket, path))
            raise


# Remove an object from a bucket, ignoring "not found" errors
def remove(bucket, path):
    try:
        bucket.delete_key(path)
    except (GSResponseError, S3ResponseError) as e:
        if e.status != 404:
            raise
        to_stderr(str(e))
        to_stderr('Ignoring path: %s%s' % (
            destination, path))


def finished(connection):
    cursor = connection.cursor()
    cursor.execute('''SELECT 1 FROM source WHERE NOT processed LIMIT 1''')
    return cursor.fetchone() is None


def process(source_bucket, destination_bucket, connection, process_pool_args):

    print 'Skipping over up-to-date objects...'
    connection.execute('''
        UPDATE source SET processed = 1
        WHERE rowid IN (
            SELECT s.rowid FROM source s JOIN destination d
            ON s.path = d.path AND s.hash = d.hash
        )
    ''')

    print 'Uploading new/updated objects from source to destination...'
    process_pool = Pool(**process_pool_args)
    while not finished(connection):
        # Use a list instead of a generator because "SQLite objects created in
        # a thread can only be used in that same thread". The query only
        # fetches a chunk of paths at a time so it's not too bad.
        paths = [row[0] for row in
                 connection.execute('''SELECT path FROM source WHERE NOT processed LIMIT 1000''')]
        resulting_paths = process_pool.imap_unordered(partial(transfer, source_bucket, destination_bucket), paths)
        for path in resulting_paths:
            connection.execute('''UPDATE source SET processed = 1 WHERE path = ?''', (path,))
            print 'Finished: %s/%s -> %s/%s' % (source_bucket, path, destination_bucket, path)

    print 'Deleting objects in destination that have been deleted in source...'
    for row in connection.execute('''
        SELECT d.rowid, d.path
        FROM destination d LEFT JOIN source s
        ON d.path = s.path WHERE s.path IS NULL
    '''):
        remove(destination_bucket, row[1])
        connection.execute('''DELETE FROM destination WHERE rowid = ?''', (row[0],))


# Populate the table with the contents of the bucket
def get_contents(bucket, folder, connection, table):
    for key in bucket.list(prefix=folder):
        connection.execute('INSERT INTO %s (bucket, path, hash) VALUES (?, ?, ?)' % table,
                           (bucket.name, key.name, key.etag))


def initialize_db(connection):
    connection.executescript('''
        CREATE TABLE source (bucket VARCHAR,
                             path VARCHAR,
                             hash VARCHAR,
                             processed BOOLEAN DEFAULT 0);
        CREATE TABLE destination (bucket VARCHAR, path VARCHAR, hash VARCHAR);
        CREATE INDEX IF NOT EXISTS source_path_index ON source (path);
        CREATE INDEX IF NOT EXISTS source_hash_index ON source (hash);
        CREATE INDEX IF NOT EXISTS destination_path_index ON destination (path);
        CREATE INDEX IF NOT EXISTS destination_hash_index ON destination (hash);
    ''')


def new_run(source_bucket, destination_bucket, folder, db, process_pool_args):
    print 'Starting a new run...'

    connection = sqlite3.connect(db, isolation_level=None)
    initialize_db(connection)

    get_contents(destination_bucket, folder, connection, 'destination')
    get_contents(source_bucket, folder, connection, 'source')

    process(source_bucket, destination_bucket, connection, process_pool_args)


def break_uri(uri):
    # scheme://bucket-name/sub/folder -> (scheme, bucket-name, /sub/folder)
    parts = uri.split('/')
    scheme = parts[0].split(':')[0]
    bucket_name = parts[2]
    folder = '/'.join(parts[3:])
    return (scheme, bucket_name, folder)


DEFAULT_DB = 'state.db'

def heavy_sync(source, destination, db=DEFAULT_DB, create_bucket_args={}, process_pool_args={}):

    s_scheme, s_bucket_name, folder = break_uri(source)
    d_scheme, d_bucket_name, _      = break_uri(destination)

    s_bucket = lookup(s_scheme, s_bucket_name)
    d_bucket = lookup(d_scheme, d_bucket_name)

    if d_bucket is None:
        d_bucket = get_connection(d_scheme).create_bucket(d_bucket_name,
                                                          **create_bucket_args)
    # TODO: make this an option?
    if not d_bucket.get_versioning_status():
        d_bucket.configure_versioning(True)

    assert s_bucket is not None
    assert d_bucket is not None

    if path.exists(db):

        try:
            connection = sqlite3.connect(db, isolation_level=None)
            if finished(connection):
                print 'Backing up previous completed run...'
                connection.close()
                rename(db, '%s-%d' % (db, int(time())))
                new_run(s_bucket, d_bucket, folder, db, process_pool_args)
            else:
                print 'Resuming a previous run...'
                process(s_bucket, d_bucket, connection, process_pool_args)

        except sqlite3.OperationalError as e:
            to_stderr(str(e))
            to_stderr('Error encountered, please clean up %s manually.' % db)
            raise

    else:
        new_run(s_bucket, d_bucket, folder, db, process_pool_args)


def maybe_copy_key(s_dict, d_dict, key):
    value = s_dict.get(key, None)
    if value is not None:
        d_dict[key] = value


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('source')
    parser.add_argument('destination')
    parser.add_argument('--db', default=DEFAULT_DB)

    parser.add_argument('--location', default=None)
    # GS only
    parser.add_argument('--storage-class', default=None)

    parser.add_argument('--processes', type=int, default=None)

    args = vars(parser.parse_args())

    create_bucket_args = {}
    maybe_copy_key(args, create_bucket_args, 'location')
    maybe_copy_key(args, create_bucket_args, 'storage_class')

    process_pool_args = {}
    maybe_copy_key(args, process_pool_args, 'processes')

    heavy_sync(args['source'], args['destination'], args['db'], create_bucket_args, process_pool_args)


if __name__ == '__main__':
    main()
